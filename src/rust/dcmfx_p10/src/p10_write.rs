//! Functionality for serializing data sets and streams of DICOM P10 tokens into
//! DICOM P10 bytes.

#[cfg(not(feature = "std"))]
use alloc::{
  boxed::Box,
  format,
  string::{String, ToString},
  vec,
  vec::Vec,
};

use byteorder::ByteOrder;

use dcmfx_core::DataSetPath;
use dcmfx_core::{
  DataElementValue, DataError, DataSet, RcByteSlice, TransferSyntax,
  dictionary, transfer_syntax, transfer_syntax::Endianness,
};

use crate::internal::p10_location::P10Location;
use crate::{
  P10Error, P10FilterTransform, P10InsertTransform, P10Token, P10WriteConfig,
  internal::{
    data_element_header::{DataElementHeader, ValueLengthSize},
    value_length::ValueLength,
  },
  p10_token,
};

/// Data is compressed into chunks of this size when writing deflated transfer
/// syntaxes.
///
const ZLIB_DEFLATE_CHUNK_SIZE: usize = 64 * 1024;

/// A write context holds the current state of an in-progress DICOM P10 write.
/// DICOM P10 tokens are written to a write context with
/// [`Self::write_token()`], and output P10 bytes are returned by
/// [`Self::read_bytes()`].
///
pub struct P10WriteContext {
  config: P10WriteConfig,
  p10_bytes: Vec<RcByteSlice>,
  p10_total_byte_count: u64,
  is_ended: bool,
  transfer_syntax: &'static TransferSyntax,
  zlib_stream: Option<flate2::Compress>,
  location: P10Location,
  path: DataSetPath,
}

impl P10WriteContext {
  /// Creates a new write context for writing DICOM P10 data.
  ///
  pub fn new(config: Option<P10WriteConfig>) -> Self {
    Self {
      config: config.unwrap_or_default(),
      p10_bytes: vec![],
      p10_total_byte_count: 0,
      is_ended: false,
      transfer_syntax: &transfer_syntax::IMPLICIT_VR_LITTLE_ENDIAN,
      zlib_stream: None,
      location: P10Location::new(),
      path: DataSetPath::new(),
    }
  }

  /// Reads the current DICOM P10 bytes available out of a write context. These
  /// are the bytes generated by recent calls to [`Self::write_token()`].
  ///
  pub fn read_bytes(&mut self) -> Vec<RcByteSlice> {
    core::mem::take(&mut self.p10_bytes)
  }

  /// Writes a DICOM P10 token to a write context. On success an updated write
  /// context is returned. Use [`Self::read_bytes()`] to get the new DICOM P10
  /// bytes generated as a result of writing this token.
  ///
  pub fn write_token(&mut self, token: &P10Token) -> Result<(), P10Error> {
    if self.is_ended {
      return Err(P10Error::TokenStreamInvalid {
        when: "Writing DICOM P10 token".to_string(),
        details:
          "Received a further DICOM P10 token after the write was completed"
            .to_string(),
        token: token.clone(),
      });
    }

    match token {
      // When the File Meta Information token is received, check it for a
      // transfer syntax value that should be put onto the write context, and
      // start a zlib compressor if the transfer syntax is deflated
      P10Token::FileMetaInformation {
        data_set: file_meta_information,
      } => {
        // Read the transfer syntax UID
        let transfer_syntax_uid = file_meta_information
          .get_string(dictionary::TRANSFER_SYNTAX_UID.tag)
          .unwrap_or(transfer_syntax::IMPLICIT_VR_LITTLE_ENDIAN.uid);

        // Map UID to a known transfer syntax
        let new_transfer_syntax = TransferSyntax::from_uid(transfer_syntax_uid)
          .map_err(|_| P10Error::TransferSyntaxNotSupported {
            transfer_syntax_uid: transfer_syntax_uid.to_string(),
          })?;

        // If this is a deflated transfer syntax then start a zlib compressor
        // and exclude the zlib header
        if new_transfer_syntax.is_deflated {
          self.zlib_stream = Some(flate2::Compress::new(
            flate2::Compression::new(self.config.zlib_compression_level),
            false,
          ));
        }

        self.transfer_syntax = new_transfer_syntax;

        let token_bytes = self.token_to_bytes(token)?;
        self.p10_total_byte_count += token_bytes.len() as u64;
        self.p10_bytes.push(token_bytes);

        Ok(())
      }

      // When the end token is received, update the flag on the write context
      // and flush all remaining data out of the zlib stream if one is in use
      P10Token::End => {
        if let Some(zlib_stream) = self.zlib_stream.as_mut() {
          loop {
            let mut output = vec![0u8; ZLIB_DEFLATE_CHUNK_SIZE];

            let total_out = zlib_stream.total_out();

            let status = zlib_stream
              .compress(
                &[],
                output.as_mut_slice(),
                flate2::FlushCompress::Finish,
              )
              .map_err(|error| P10Error::DataInvalid {
                when: "Performing zlib compression".to_string(),
                details: error.message().unwrap_or("<unknown>").to_string(),
                path: self.path.clone(),
                offset: self.p10_total_byte_count,
              })?;

            output.resize((zlib_stream.total_out() - total_out) as usize, 0u8);

            if !output.is_empty() {
              self.p10_total_byte_count += output.len() as u64;
              self.p10_bytes.push(output.into());
            }

            if status == flate2::Status::StreamEnd {
              break;
            }
          }

          self.zlib_stream = None;
        }

        self.is_ended = true;

        Ok(())
      }

      _ => {
        let map_to_p10_token_stream_error =
          |details: String| P10Error::TokenStreamInvalid {
            when: "Writing token to context".to_string(),
            details,
            token: token.clone(),
          };

        // Update the current location
        match token {
          P10Token::DataElementHeader { tag, .. } => {
            self.path.add_data_element(*tag)
          }

          P10Token::SequenceStart { tag, .. } => self
            .location
            .add_sequence(*tag, false, None)
            .and_then(|_| self.path.add_data_element(*tag)),

          P10Token::SequenceItemStart { .. }
          | P10Token::PixelDataItem { .. } => self
            .location
            .add_item(None, ValueLength::Undefined)
            .and_then(|index| self.path.add_sequence_item(index)),

          _ => Ok(()),
        }
        .map_err(map_to_p10_token_stream_error)?;

        // Convert token to bytes
        let token_bytes = self.token_to_bytes(token)?;

        // Update the current location
        match token {
          P10Token::DataElementValueBytes {
            tag,
            vr,
            bytes_remaining: 0,
            data,
          } => {
            if *tag == dictionary::BITS_ALLOCATED.tag
              || *tag == dictionary::WAVEFORM_BITS_ALLOCATED.tag
            {
              let mut data = (*data).clone();
              self
                .location
                .add_clarifying_data_element(*tag, *vr, &mut data)?;
            }

            if *tag == dictionary::ITEM.tag {
              self.location.end_item().and_then(|_| self.path.pop())
            } else {
              self.path.pop()
            }
          }

          P10Token::SequenceItemDelimiter => {
            self.location.end_item().and_then(|_| self.path.pop())
          }

          P10Token::SequenceDelimiter { .. } => {
            self.location.end_sequence().and_then(|_| self.path.pop())
          }

          _ => Ok(&mut self.path),
        }
        .map_err(map_to_p10_token_stream_error)?;

        // If a zlib stream is active then pass the P10 bytes through it
        if let Some(zlib_stream) = self.zlib_stream.as_mut() {
          let mut token_bytes_remaining = &token_bytes[..];

          while !token_bytes_remaining.is_empty() {
            let mut output = vec![0u8; ZLIB_DEFLATE_CHUNK_SIZE];

            // Add bytes to the zlib compressor and read back any compressed
            // data
            let total_in = zlib_stream.total_in();
            let total_out = zlib_stream.total_out();
            zlib_stream
              .compress(
                token_bytes_remaining,
                &mut output,
                flate2::FlushCompress::None,
              )
              .unwrap();
            output.resize((zlib_stream.total_out() - total_out) as usize, 0u8);

            if !output.is_empty() {
              self.p10_total_byte_count += output.len() as u64;
              self.p10_bytes.push(output.into());
            }

            let input_bytes_consumed =
              (zlib_stream.total_in() - total_in) as usize;
            if input_bytes_consumed == 0 {
              panic!("zlib compressor did not consume any bytes");
            }

            token_bytes_remaining =
              &token_bytes_remaining[input_bytes_consumed..];
          }
        } else {
          self.p10_total_byte_count += token_bytes.len() as u64;
          self.p10_bytes.push(token_bytes);
        }

        Ok(())
      }
    }
  }

  /// Converts a single DICOM P10 token to raw DICOM P10 bytes.
  ///
  fn token_to_bytes(&self, token: &P10Token) -> Result<RcByteSlice, P10Error> {
    match token {
      P10Token::FilePreambleAndDICMPrefix { preamble } => {
        let mut data = Vec::with_capacity(132);

        data.extend_from_slice(preamble.as_ref());
        data.extend_from_slice(b"DICM");

        Ok(data.into())
      }

      P10Token::FileMetaInformation { data_set } => {
        let mut file_meta_information = data_set.clone();
        prepare_file_meta_information_token_data_set(
          &mut file_meta_information,
          &self.config.implementation_class_uid,
          &self.config.implementation_version_name,
        )
        .map_err(|e| P10Error::DataInvalid {
          when: "Serializing File Meta Information".to_string(),
          details: e.details().to_string(),
          path: e.path().cloned().unwrap_or_default(),
          offset: self.p10_total_byte_count,
        })?;

        let mut fmi_bytes = Vec::with_capacity(1024);

        // Set the File Meta Information Group Length, with a placeholder for
        // the 32-bit length at the end. The length will be filled in once the
        // rest of the FMI bytes have been created.
        fmi_bytes
          .extend_from_slice(&[0x02, 0x00, 0x00, 0x00, 0x55, 0x4C, 0x04, 0x00]);
        fmi_bytes.extend_from_slice(&[0, 0, 0, 0]);

        for (tag, value) in file_meta_information.into_iter() {
          let vr = value.value_representation();

          let value_bytes =
            value.bytes().map_err(|_| P10Error::DataInvalid {
              when: "Serializing File Meta Information".to_string(),
              details: format!(
            "Tag '{}' with value representation '{}' is not allowed in File \
              Meta Information",
            tag, vr
          ),
              path: self.path.clone(),
              offset: self.p10_total_byte_count,
            })?;

          let header_bytes = self.data_element_header_to_bytes(
            &DataElementHeader {
              tag,
              vr: Some(vr),
              length: ValueLength::new(value_bytes.len() as u32),
            },
            transfer_syntax::Endianness::LittleEndian,
          )?;

          fmi_bytes.extend_from_slice(&header_bytes);
          fmi_bytes.extend_from_slice(value_bytes);
        }

        // Set the final File Meta Information Group Length value
        let fmi_length = fmi_bytes.len() - 12;
        byteorder::LittleEndian::write_u32_into(
          &[fmi_length as u32],
          &mut fmi_bytes[8..12],
        );

        Ok(fmi_bytes.into())
      }

      P10Token::DataElementHeader {
        tag, vr, length, ..
      } => {
        let vr = match self.transfer_syntax.vr_serialization {
          transfer_syntax::VrSerialization::VrExplicit => Some(*vr),
          transfer_syntax::VrSerialization::VrImplicit => None,
        };

        self.data_element_header_to_bytes(
          &DataElementHeader {
            tag: *tag,
            vr,
            length: ValueLength::new(*length),
          },
          self.transfer_syntax.endianness,
        )
      }

      P10Token::DataElementValueBytes { tag, vr, data, .. } => {
        if self.transfer_syntax.endianness.is_big() {
          // To swap endianness the data needs to be cloned as it can't be
          // swapped in place
          let mut data = data.to_vec();
          self.location.swap_endianness(*tag, *vr, &mut data);

          Ok(data.into())
        } else {
          Ok(data.clone())
        }
      }

      P10Token::SequenceStart { tag, vr, .. } => {
        let vr = match self.transfer_syntax.vr_serialization {
          transfer_syntax::VrSerialization::VrExplicit => Some(*vr),
          transfer_syntax::VrSerialization::VrImplicit => None,
        };

        self.data_element_header_to_bytes(
          &DataElementHeader {
            tag: *tag,
            vr,
            length: ValueLength::Undefined,
          },
          self.transfer_syntax.endianness,
        )
      }

      P10Token::SequenceDelimiter { .. } => self.data_element_header_to_bytes(
        &DataElementHeader {
          tag: dictionary::SEQUENCE_DELIMITATION_ITEM.tag,
          vr: None,
          length: ValueLength::ZERO,
        },
        self.transfer_syntax.endianness,
      ),

      P10Token::SequenceItemStart { .. } => self.data_element_header_to_bytes(
        &DataElementHeader {
          tag: dictionary::ITEM.tag,
          vr: None,
          length: ValueLength::Undefined,
        },
        self.transfer_syntax.endianness,
      ),

      P10Token::SequenceItemDelimiter => self.data_element_header_to_bytes(
        &DataElementHeader {
          tag: dictionary::ITEM_DELIMITATION_ITEM.tag,
          vr: None,
          length: ValueLength::ZERO,
        },
        self.transfer_syntax.endianness,
      ),

      P10Token::PixelDataItem { length, .. } => self
        .data_element_header_to_bytes(
          &DataElementHeader {
            tag: dictionary::ITEM.tag,
            vr: None,
            length: ValueLength::new(*length),
          },
          self.transfer_syntax.endianness,
        ),

      P10Token::End => Ok(RcByteSlice::empty()),
    }
  }

  /// Serializes a data element header to a `Vec<u8>`. If a VR is not supplied
  /// then implicit VR encoding will be used.
  ///
  fn data_element_header_to_bytes(
    &self,
    header: &DataElementHeader,
    endianness: Endianness,
  ) -> Result<RcByteSlice, P10Error> {
    let length = header.length.to_u32();

    let mut bytes = Vec::with_capacity(12);

    match endianness {
      Endianness::LittleEndian => {
        bytes.extend_from_slice(header.tag.group.to_le_bytes().as_slice());
        bytes.extend_from_slice(header.tag.element.to_le_bytes().as_slice());
      }
      Endianness::BigEndian => {
        bytes.extend_from_slice(header.tag.group.to_be_bytes().as_slice());
        bytes.extend_from_slice(header.tag.element.to_be_bytes().as_slice());
      }
    };

    match header.vr {
      // Write with implicit VR
      None => match endianness {
        Endianness::LittleEndian => {
          bytes.extend_from_slice(length.to_le_bytes().as_slice())
        }
        Endianness::BigEndian => {
          bytes.extend_from_slice(length.to_be_bytes().as_slice())
        }
      },

      // Write with explicit VR
      Some(vr) => {
        bytes.extend_from_slice(vr.to_string().as_bytes());

        match DataElementHeader::value_length_size(vr) {
          // All other VRs use a 16-bit length. Check that the data length fits
          // inside this constraint.
          ValueLengthSize::U16 => {
            if length > u16::MAX.into() {
              return Err(P10Error::DataInvalid {
                when: "Serializing data element header".to_string(),
                details: format!(
                  "Length {} exceeds the maximum of 2^16 - 1 bytes",
                  header.length.to_u32(),
                ),
                path: self.path.clone(),
                offset: self.p10_total_byte_count,
              });
            }

            match endianness {
              Endianness::LittleEndian => bytes
                .extend_from_slice((length as u16).to_le_bytes().as_slice()),
              Endianness::BigEndian => bytes
                .extend_from_slice((length as u16).to_be_bytes().as_slice()),
            }
          }

          // The following VRs use a 32-bit length preceded by two padding bytes
          ValueLengthSize::U32 => {
            bytes.extend_from_slice([0, 0].as_slice());

            match endianness {
              Endianness::LittleEndian => {
                bytes.extend_from_slice(length.to_le_bytes().as_slice())
              }
              Endianness::BigEndian => {
                bytes.extend_from_slice(length.to_be_bytes().as_slice())
              }
            }
          }
        };
      }
    }

    Ok(bytes.into())
  }
}

impl Default for P10WriteContext {
  fn default() -> Self {
    Self::new(None)
  }
}

/// Converts a data set to DICOM P10 tokens. The generated P10 tokens are
/// returned via a callback.
///
pub fn data_set_to_tokens<E>(
  data_set: &DataSet,
  path: &DataSetPath,
  token_callback: &mut impl FnMut(&P10Token) -> Result<(), E>,
) -> Result<(), E> {
  // Create filter transform that removes File Meta Information data elements
  // from the data set's token stream
  let mut remove_fmi_transform =
    P10FilterTransform::new(Box::new(|tag, _vr, _length, path| {
      !path.is_root() || !tag.is_file_meta_information()
    }));

  // Create insert transform to add the '(0008,0005) SpecificCharacterSet' data
  // element into the data set's token stream, specifying UTF-8 (ISO_IR 192)
  let mut data_elements_to_insert = DataSet::new();
  data_elements_to_insert
    .insert_string_value(&dictionary::SPECIFIC_CHARACTER_SET, &["ISO_IR 192"])
    .unwrap();
  let mut insert_specific_character_set_transform =
    P10InsertTransform::new(data_elements_to_insert);

  // Create a function that passes token through the above two transforms and
  // then to the callback
  let mut process_token = |token: &P10Token| -> Result<(), E> {
    // The following two unwraps are safe because the P10 transforms only error
    // on invalid token streams, which can't happen here

    if !remove_fmi_transform.add_token(token).unwrap() {
      return Ok(());
    }

    let tokens = insert_specific_character_set_transform
      .add_token(token)
      .unwrap();

    for token in tokens {
      token_callback(&token)?;
    }

    Ok(())
  };

  // Write File Preamble and File Meta Information tokens
  let preamble_token = P10Token::FilePreambleAndDICMPrefix {
    preamble: Box::new([0; 128]),
  };
  process_token(&preamble_token)?;
  let fmi_token = P10Token::FileMetaInformation {
    data_set: data_set.file_meta_information(),
  };
  process_token(&fmi_token)?;

  // Write main data set
  p10_token::data_elements_to_tokens(data_set, path, &mut process_token)?;

  // Write end token
  process_token(&P10Token::End)
}

/// Converts a data set to DICOM P10 bytes. The generated P10 bytes are returned
/// via a callback.
///
pub fn data_set_to_bytes(
  data_set: &DataSet,
  path: &DataSetPath,
  bytes_callback: &mut impl FnMut(RcByteSlice) -> Result<(), P10Error>,
  config: Option<P10WriteConfig>,
) -> Result<(), P10Error> {
  let mut context = P10WriteContext::new(config);

  let mut process_token = |token: &P10Token| -> Result<(), P10Error> {
    context.write_token(token)?;

    let p10_bytes = context.read_bytes();
    for bytes in p10_bytes {
      bytes_callback(bytes)?;
    }

    Ok(())
  };

  data_set_to_tokens(data_set, path, &mut process_token)
}

/// Sets the *'(0002,0001) File Meta Information Version'*, *'(0002,0012)
/// Implementation Class UID'* and *'(0002,0013) Implementation Version Name'*
/// values in the File Meta Information. This is done prior to serializing it
/// to bytes.
///
fn prepare_file_meta_information_token_data_set(
  file_meta_information: &mut DataSet,
  implementation_class_uid: &str,
  implementation_version_name: &str,
) -> Result<(), DataError> {
  let file_meta_information_version =
    DataElementValue::new_other_byte_string(vec![0, 1]).unwrap();

  file_meta_information.insert(
    dictionary::FILE_META_INFORMATION_VERSION.tag,
    file_meta_information_version,
  );
  file_meta_information.insert_string_value(
    &dictionary::IMPLEMENTATION_CLASS_UID,
    &[implementation_class_uid],
  )?;
  file_meta_information.insert_string_value(
    &dictionary::IMPLEMENTATION_VERSION_NAME,
    &[implementation_version_name],
  )
}

#[cfg(test)]
mod tests {
  use super::*;

  use dcmfx_core::ValueRepresentation;

  #[test]
  fn data_element_header_to_bytes_test() {
    assert_eq!(
      P10WriteContext::new(None).data_element_header_to_bytes(
        &DataElementHeader {
          tag: dictionary::WAVEFORM_DATA.tag,
          vr: None,
          length: ValueLength::new(0x12345678),
        },
        Endianness::LittleEndian,
      ),
      Ok(vec![0, 84, 16, 16, 120, 86, 52, 18].into())
    );

    assert_eq!(
      P10WriteContext::new(None).data_element_header_to_bytes(
        &DataElementHeader {
          tag: dictionary::WAVEFORM_DATA.tag,
          vr: None,
          length: ValueLength::new(0x12345678),
        },
        Endianness::BigEndian,
      ),
      Ok(vec![84, 0, 16, 16, 18, 52, 86, 120].into())
    );

    assert_eq!(
      P10WriteContext::new(None).data_element_header_to_bytes(
        &DataElementHeader {
          tag: dictionary::PATIENT_AGE.tag,
          vr: Some(ValueRepresentation::UnlimitedText),
          length: ValueLength::new(0x1234),
        },
        Endianness::LittleEndian,
      ),
      Ok(vec![16, 0, 16, 16, 85, 84, 0, 0, 52, 18, 0, 0].into())
    );

    assert_eq!(
      P10WriteContext::new(None).data_element_header_to_bytes(
        &DataElementHeader {
          tag: dictionary::PIXEL_DATA.tag,
          vr: Some(ValueRepresentation::OtherWordString),
          length: ValueLength::new(0x12345678),
        },
        Endianness::LittleEndian,
      ),
      Ok(vec![224, 127, 16, 0, 79, 87, 0, 0, 120, 86, 52, 18].into())
    );

    assert_eq!(
      P10WriteContext::new(None).data_element_header_to_bytes(
        &DataElementHeader {
          tag: dictionary::PIXEL_DATA.tag,
          vr: Some(ValueRepresentation::OtherWordString),
          length: ValueLength::new(0x12345678),
        },
        Endianness::BigEndian,
      ),
      Ok(vec![127, 224, 0, 16, 79, 87, 0, 0, 18, 52, 86, 120].into())
    );

    assert_eq!(
      P10WriteContext::new(None).data_element_header_to_bytes(
        &DataElementHeader {
          tag: dictionary::PATIENT_AGE.tag,
          vr: Some(ValueRepresentation::AgeString),
          length: ValueLength::new(74565),
        },
        Endianness::LittleEndian,
      ),
      Err(P10Error::DataInvalid {
        when: "Serializing data element header".to_string(),
        details: "Length 74565 exceeds the maximum of 2^16 - 1 bytes"
          .to_string(),
        path: DataSetPath::new(),
        offset: 0
      })
    );

    assert_eq!(
      P10WriteContext::new(None).data_element_header_to_bytes(
        &DataElementHeader {
          tag: dictionary::SMALLEST_IMAGE_PIXEL_VALUE.tag,
          vr: Some(ValueRepresentation::SignedShort),
          length: ValueLength::new(0x1234),
        },
        Endianness::LittleEndian,
      ),
      Ok(vec![40, 0, 6, 1, 83, 83, 52, 18].into())
    );

    assert_eq!(
      P10WriteContext::new(None).data_element_header_to_bytes(
        &DataElementHeader {
          tag: dictionary::SMALLEST_IMAGE_PIXEL_VALUE.tag,
          vr: Some(ValueRepresentation::SignedShort),
          length: ValueLength::new(0x1234),
        },
        Endianness::BigEndian,
      ),
      Ok(vec![0, 40, 1, 6, 83, 83, 18, 52].into())
    );
  }
}
