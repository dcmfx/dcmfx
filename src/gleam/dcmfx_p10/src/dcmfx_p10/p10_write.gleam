//// Functionality for serializing data sets and streams of DICOM P10 tokens
//// into DICOM P10 bytes.

import dcmfx_core/data_element_tag
import dcmfx_core/data_element_value
import dcmfx_core/data_error
import dcmfx_core/data_set.{type DataSet}
import dcmfx_core/data_set_path.{type DataSetPath}
import dcmfx_core/dictionary
import dcmfx_core/transfer_syntax.{
  type Endianness, type TransferSyntax, BigEndian, LittleEndian,
}
import dcmfx_core/value_representation
import dcmfx_p10/internal/data_element_header.{
  type DataElementHeader, DataElementHeader,
}
import dcmfx_p10/internal/p10_location.{type P10Location}
import dcmfx_p10/internal/value_length
import dcmfx_p10/internal/zlib.{type ZlibStream}
import dcmfx_p10/internal/zlib/flush_command
import dcmfx_p10/p10_error.{type P10Error}
import dcmfx_p10/p10_token.{type P10Token}
import dcmfx_p10/transforms/p10_filter_transform.{type P10FilterTransform}
import dcmfx_p10/transforms/p10_insert_transform.{type P10InsertTransform}
import dcmfx_p10/uids
import gleam/bit_array
import gleam/bool
import gleam/int
import gleam/list
import gleam/option.{type Option, None, Some}
import gleam/pair
import gleam/result

/// Configuration used when writing DICOM P10 data. The following config is
/// available:
///
/// ### `implementation_class_uid: String`
///
/// The implementation class UID that will be included in the File Meta
/// Information header of serialized DICOM P10 data.
///
/// Defaults to the value of `dcmfx_p10/uids.dcmfx_implementation_class_uid`.
///
/// ### `implementation_version_name: String`
///
/// The implementation version name that will be included in the File Meta
/// Information header of serialized DICOM P10 data.
///
/// Defaults to the value of `dcmfx_p10/uids.dcmfx_implementation_version_name`.
///
/// ### `zlib_compression_level: Int`
///
/// The zlib compression level to use when the transfer syntax being used is
/// deflated. There are only three deflated transfer syntaxes: 'Deflated
/// Explicit VR Little Endian', 'JPIP Referenced Deflate', and 'JPIP HTJ2K
/// Referenced Deflate'.
///
/// The level ranges from 0, meaning no compression, through to 9, which gives
/// the best compression at the cost of speed.
///
/// Default: 6.
///
pub type P10WriteConfig {
  P10WriteConfig(
    implementation_class_uid: String,
    implementation_version_name: String,
    zlib_compression_level: Int,
  )
}

/// Returns the default write config.
///
pub fn default_config() -> P10WriteConfig {
  P10WriteConfig(
    implementation_class_uid: uids.dcmfx_implementation_class_uid,
    implementation_version_name: uids.dcmfx_implementation_version_name,
    zlib_compression_level: 6,
  )
}

/// A write context holds the current state of an in-progress DICOM P10 write.
/// DICOM P10 tokens are written to a write context with `write_token()`, and
/// output P10 bytes are returned by `read_bytes()`.
///
pub opaque type P10WriteContext {
  P10WriteContext(
    config: P10WriteConfig,
    p10_bytes: List(BitArray),
    p10_total_byte_count: Int,
    is_ended: Bool,
    transfer_syntax: TransferSyntax,
    zlib_stream: Option(ZlibStream),
    location: P10Location,
    path: DataSetPath,
  )
}

/// Creates a new write context for writing DICOM P10 data.
///
pub fn new_write_context() -> P10WriteContext {
  P10WriteContext(
    config: default_config(),
    p10_bytes: [],
    p10_total_byte_count: 0,
    is_ended: False,
    transfer_syntax: transfer_syntax.implicit_vr_little_endian,
    zlib_stream: None,
    location: p10_location.new(),
    path: data_set_path.new(),
  )
}

/// Updates the config for a write context.
///
pub fn with_config(
  context: P10WriteContext,
  config: P10WriteConfig,
) -> P10WriteContext {
  // Clamp zlib compression level to the valid range
  let config =
    P10WriteConfig(
      ..config,
      zlib_compression_level: int.clamp(config.zlib_compression_level, 0, 9),
    )

  P10WriteContext(..context, config: config)
}

/// Reads the current DICOM P10 bytes available out of a write context. These
/// are the bytes generated by recent calls to `write_token()`.
///
pub fn read_bytes(
  context: P10WriteContext,
) -> #(List(BitArray), P10WriteContext) {
  let p10_bytes = list.reverse(context.p10_bytes)

  #(p10_bytes, P10WriteContext(..context, p10_bytes: []))
}

/// Writes a DICOM P10 token to a write context. On success an updated write
/// context is returned. Use `read_bytes()` to get the new DICOM P10 bytes
/// generated as a result of writing this token.
///
pub fn write_token(
  context: P10WriteContext,
  token: P10Token,
) -> Result(P10WriteContext, P10Error) {
  use <- bool.guard(
    context.is_ended,
    Error(p10_error.TokenStreamInvalid(
      when: "Writing DICOM P10 token",
      details: "Received a further DICOM P10 token after the write was "
        <> "completed",
      token:,
    )),
  )

  case token {
    // When the File Meta Information token is received, check it for a transfer
    // syntax value that should be put onto the write context, and start a zlib
    // compressor if the transfer syntax is deflated
    p10_token.FileMetaInformation(file_meta_information) -> {
      // Read the transfer syntax UID
      let transfer_syntax_uid =
        file_meta_information
        |> data_set.get_string(dictionary.transfer_syntax_uid.tag)
        |> result.unwrap(transfer_syntax.implicit_vr_little_endian.uid)

      // Map UID to a known transfer syntax
      let new_transfer_syntax =
        transfer_syntax_uid
        |> transfer_syntax.from_uid
        |> result.map_error(fn(_) {
          p10_error.TransferSyntaxNotSupported(transfer_syntax_uid)
        })
      use new_transfer_syntax <- result.try(new_transfer_syntax)

      // If this is a deflated transfer syntax then start a zlib compressor.
      // The window bits of -15 stops the zlib header and checksum appearing in
      // the output.
      let zlib_stream = case new_transfer_syntax.is_deflated {
        True -> {
          let stream = zlib.open()
          zlib.deflate_init(
            stream,
            context.config.zlib_compression_level,
            zlib.Deflated,
            -15,
            8,
            zlib.Default,
          )

          Some(stream)
        }

        _ -> None
      }

      let new_context =
        P10WriteContext(
          ..context,
          transfer_syntax: new_transfer_syntax,
          zlib_stream: zlib_stream,
        )

      use token_bytes <- result.map(token_to_bytes(token, new_context))

      P10WriteContext(
        ..new_context,
        p10_bytes: [token_bytes, ..new_context.p10_bytes],
        p10_total_byte_count: context.p10_total_byte_count
          + bit_array.byte_size(token_bytes),
      )
    }

    // When the end token is received, update the flag on the write context and
    // flush all remaining data out of the zlib stream if one is in use
    p10_token.End ->
      case context.zlib_stream {
        Some(zlib_stream) -> {
          let data =
            zlib_stream
            |> zlib.deflate(<<>>, flush_command.Finish)
            |> bit_array.concat

          P10WriteContext(
            ..context,
            p10_bytes: [data, ..context.p10_bytes],
            p10_total_byte_count: context.p10_total_byte_count
              + bit_array.byte_size(data),
            is_ended: True,
            zlib_stream: None,
          )
          |> Ok
        }

        None -> Ok(P10WriteContext(..context, is_ended: True))
      }

    _ -> {
      let map_to_p10_token_stream_error = result.map_error(_, fn(details) {
        p10_error.TokenStreamInvalid(
          when: "Writing token to context",
          details:,
          token:,
        )
      })

      // Update the current location
      let context =
        case token {
          p10_token.DataElementHeader(tag:, ..) -> {
            let path = data_set_path.add_data_element(context.path, tag)
            use path <- result.try(path)

            Ok(P10WriteContext(..context, path:))
          }

          p10_token.SequenceStart(tag:, ..) -> {
            let location =
              context.location
              |> p10_location.add_sequence(tag, False, None)
            use location <- result.try(location)

            let path = data_set_path.add_data_element(context.path, tag)
            use path <- result.try(path)

            Ok(P10WriteContext(..context, location:, path:))
          }

          p10_token.SequenceItemStart(..) | p10_token.PixelDataItem(..) -> {
            let location =
              context.location
              |> p10_location.add_item(None, value_length.Undefined)
            use #(item_index, location) <- result.try(location)

            let path = data_set_path.add_sequence_item(context.path, item_index)
            use path <- result.try(path)

            Ok(P10WriteContext(..context, location:, path:))
          }

          _ -> Ok(context)
        }
        |> map_to_p10_token_stream_error

      use context <- result.try(context)

      // Convert token to bytes
      use token_bytes <- result.try(token_to_bytes(token, context))

      // Update the current location
      let context = case token {
        p10_token.DataElementValueBytes(tag:, vr:, bytes_remaining: 0, data:) -> {
          let is_pixel_or_waveform_data =
            tag == dictionary.bits_allocated.tag
            || tag == dictionary.waveform_bits_allocated.tag

          let location = case is_pixel_or_waveform_data {
            True ->
              p10_location.add_clarifying_data_element(
                context.location,
                tag,
                vr,
                data,
              )
              |> result.map(pair.second)
            False -> Ok(context.location)
          }
          use location <- result.try(location)

          {
            let location = case tag == dictionary.item.tag {
              True -> p10_location.end_item(location)
              False -> Ok(location)
            }
            use location <- result.try(location)

            let path = data_set_path.pop(context.path)
            use path <- result.try(path)

            Ok(P10WriteContext(..context, location:, path:))
          }
          |> map_to_p10_token_stream_error
        }

        p10_token.SequenceItemDelimiter ->
          {
            let location = p10_location.end_item(context.location)
            use location <- result.try(location)

            let path = data_set_path.pop(context.path)
            use path <- result.try(path)

            Ok(P10WriteContext(..context, location:, path:))
          }
          |> map_to_p10_token_stream_error

        p10_token.SequenceDelimiter(..) ->
          {
            let location = p10_location.end_sequence(context.location)
            use #(_, location) <- result.try(location)

            let path = data_set_path.pop(context.path)
            use path <- result.try(path)

            Ok(P10WriteContext(..context, location:, path:))
          }
          |> map_to_p10_token_stream_error

        _ -> Ok(context)
      }
      use context <- result.map(context)

      // If a zlib stream is active then pass the P10 bytes through it
      case context.zlib_stream {
        Some(zlib_stream) -> {
          // Add bytes to the zlib compressor and read back any compressed data
          let data =
            zlib_stream
            |> zlib.deflate(token_bytes, flush_command.None)
            |> bit_array.concat

          P10WriteContext(
            ..context,
            p10_bytes: [data, ..context.p10_bytes],
            p10_total_byte_count: context.p10_total_byte_count
              + bit_array.byte_size(data),
          )
        }

        None ->
          P10WriteContext(
            ..context,
            p10_bytes: [token_bytes, ..context.p10_bytes],
            p10_total_byte_count: context.p10_total_byte_count
              + bit_array.byte_size(token_bytes),
          )
      }
    }
  }
}

/// Converts a single DICOM P10 token to raw DICOM P10 bytes.
///
fn token_to_bytes(
  token: P10Token,
  context: P10WriteContext,
) -> Result(BitArray, P10Error) {
  let transfer_syntax = context.transfer_syntax

  case token {
    p10_token.FilePreambleAndDICMPrefix(preamble) -> {
      let preamble_length = bit_array.byte_size(preamble)

      case preamble_length {
        128 -> Ok(bit_array.append(preamble, <<"DICM">>))
        _ ->
          Error(p10_error.DataInvalid(
            "Serializing File Preamble",
            "Preamble data must be 128 bytes in length but "
              <> int.to_string(preamble_length)
              <> " bytes were supplied",
            context.path,
            context.p10_total_byte_count,
          ))
      }
    }

    p10_token.FileMetaInformation(file_meta_information) -> {
      let file_meta_information =
        file_meta_information
        |> prepare_file_meta_information_token_data_set(
          context.config.implementation_class_uid,
          context.config.implementation_version_name,
        )
        |> result.map_error(fn(e) {
          p10_error.DataInvalid(
            when: "Serializing File Meta Information",
            details: data_error.details(e),
            path: data_error.path(e) |> option.unwrap(data_set_path.new()),
            offset: context.p10_total_byte_count,
          )
        })
      use file_meta_information <- result.try(file_meta_information)

      let fmi_bytes =
        file_meta_information
        |> data_set.map(fn(tag, value) {
          let vr = data_element_value.value_representation(value)

          let value_bytes =
            value
            |> data_element_value.bytes
            |> result.replace_error(p10_error.DataInvalid(
              when: "Serializing File Meta Information",
              details: "Tag '"
                <> data_element_tag.to_string(tag)
                <> "' with value representation '"
                <> value_representation.to_string(vr)
                <> "' is not allowed in File Meta Information",
              path: context.path,
              offset: context.p10_total_byte_count,
            ))
          use value_bytes <- result.try(value_bytes)

          let value_length = bit_array.byte_size(value_bytes)

          let header_bytes =
            DataElementHeader(tag, Some(vr), value_length.new(value_length))
            |> data_element_header_to_bytes(LittleEndian, context)
          use header_bytes <- result.try(header_bytes)

          Ok(bit_array.append(header_bytes, value_bytes))
        })
        |> result.all
        |> result.map(bit_array.concat)

      use fmi_bytes <- result.map(fmi_bytes)

      // Construct the File Meta Information Group Length data
      let fmi_length = bit_array.byte_size(fmi_bytes)
      let fmi_length_bytes = <<
        2:16-little,
        0:16,
        "UL",
        4:16-little,
        fmi_length:32-little,
      >>

      bit_array.concat([fmi_length_bytes, fmi_bytes])
    }

    p10_token.DataElementHeader(tag, vr, length, ..) -> {
      let vr = case transfer_syntax.vr_serialization {
        transfer_syntax.VrExplicit -> Some(vr)
        transfer_syntax.VrImplicit -> None
      }

      DataElementHeader(tag, vr, value_length.new(length))
      |> data_element_header_to_bytes(transfer_syntax.endianness, context)
    }

    p10_token.DataElementValueBytes(tag:, vr:, data:, ..) ->
      case transfer_syntax.endianness {
        LittleEndian -> data
        BigEndian ->
          p10_location.swap_endianness(context.location, tag, vr, data)
      }
      |> Ok

    p10_token.SequenceStart(tag, vr, ..) -> {
      let vr = case transfer_syntax.vr_serialization {
        transfer_syntax.VrExplicit -> Some(vr)
        transfer_syntax.VrImplicit -> None
      }

      DataElementHeader(tag, vr, value_length.Undefined)
      |> data_element_header_to_bytes(transfer_syntax.endianness, context)
    }

    p10_token.SequenceDelimiter(..) ->
      DataElementHeader(
        dictionary.sequence_delimitation_item.tag,
        None,
        value_length.zero,
      )
      |> data_element_header_to_bytes(transfer_syntax.endianness, context)

    p10_token.SequenceItemStart(..) ->
      DataElementHeader(dictionary.item.tag, None, value_length.Undefined)
      |> data_element_header_to_bytes(transfer_syntax.endianness, context)

    p10_token.SequenceItemDelimiter ->
      DataElementHeader(
        dictionary.item_delimitation_item.tag,
        None,
        value_length.zero,
      )
      |> data_element_header_to_bytes(transfer_syntax.endianness, context)

    p10_token.PixelDataItem(length:, ..) ->
      DataElementHeader(dictionary.item.tag, None, value_length.new(length))
      |> data_element_header_to_bytes(transfer_syntax.endianness, context)

    p10_token.End -> Ok(<<>>)
  }
}

/// Serializes a data element header to a `BitArray`. If a VR is not supplied
/// then implicit VR encoding will be used.
///
@internal
pub fn data_element_header_to_bytes(
  header: DataElementHeader,
  endianness: Endianness,
  context: P10WriteContext,
) -> Result(BitArray, P10Error) {
  let length = value_length.to_int(header.length)

  use <- bool.guard(
    length < 0,
    Error(p10_error.DataInvalid(
      "Serializing data element header",
      "Length is negative",
      context.path,
      context.p10_total_byte_count,
    )),
  )

  let tag_bytes = case endianness {
    LittleEndian -> <<header.tag.group:16-little, header.tag.element:16-little>>
    BigEndian -> <<header.tag.group:16-big, header.tag.element:16-big>>
  }

  case header.vr {
    // Write with implicit VR
    None ->
      case endianness {
        LittleEndian -> Ok(<<tag_bytes:bits, length:32-little>>)
        BigEndian -> Ok(<<tag_bytes:bits, length:32-big>>)
      }

    // Write with explicit VR
    Some(vr) -> {
      let length = value_length.to_int(header.length)

      let length_bytes = case data_element_header.value_length_size(vr) {
        data_element_header.ValueLengthU16 ->
          case length > 0xFFFF {
            True ->
              p10_error.DataInvalid(
                "Serializing data element header",
                "Length "
                  <> int.to_string(length)
                  <> " exceeds the maximum of 2^16 - 1 bytes",
                context.path,
                context.p10_total_byte_count,
              )
              |> Error

            False ->
              case endianness {
                LittleEndian -> Ok(<<length:16-little>>)
                BigEndian -> Ok(<<length:16-big>>)
              }
          }

        data_element_header.ValueLengthU32 ->
          case length > 0xFFFFFFFF {
            True ->
              p10_error.DataInvalid(
                "Serializing data element header",
                "Length "
                  <> int.to_string(length)
                  <> " exceeds the maximum of 0xFFFFFFFF",
                context.path,
                context.p10_total_byte_count,
              )
              |> Error

            False ->
              case endianness {
                LittleEndian -> Ok(<<0, 0, length:32-little>>)
                BigEndian -> Ok(<<0, 0, length:32-big>>)
              }
          }
      }

      use length_bytes <- result.try(length_bytes)

      Ok(<<
        tag_bytes:bits,
        value_representation.to_string(vr):utf8,
        length_bytes:bits,
      >>)
    }
  }
}

/// Converts a data set to DICOM P10 tokens. The generated P10 tokens are
/// returned via a callback.
///
pub fn data_set_to_tokens(
  data_set: DataSet,
  path: DataSetPath,
  callback_context: a,
  token_callback: fn(a, P10Token) -> Result(a, e),
) -> Result(a, e) {
  // Create filter transform that removes File Meta Information data elements
  // from the data set's token stream
  let remove_fmi_transform =
    p10_filter_transform.new(fn(tag, _vr, _length, _location) { tag.group != 2 })

  // Create insert transform to add the '(0008,0005) SpecificCharacterSet' data
  // element into the data set's token stream, specifying UTF-8 (ISO_IR 192)
  let assert Ok(data_elements_to_insert) =
    data_set.new()
    |> data_set.insert_string_value(dictionary.specific_character_set, [
      "ISO_IR 192",
    ])
  let insert_specific_character_set_transform =
    p10_insert_transform.new(data_elements_to_insert)

  // Create a function that passes tokens through the above two transforms and
  // then to the callback
  let process_token = fn(
    context: #(a, P10FilterTransform, P10InsertTransform),
    token: P10Token,
  ) {
    // The following two asserts are safe because the P10 transforms only error
    // on invalid token streams, which can't happen here

    let assert Ok(#(filter_result, filter_transform)) =
      p10_filter_transform.add_token(context.1, token)

    case filter_result {
      False -> Ok(#(context.0, filter_transform, context.2))

      True -> {
        let assert Ok(#(tokens, insert_transform)) =
          p10_insert_transform.add_token(context.2, token)

        use callback_context <- result.try(list.try_fold(
          tokens,
          context.0,
          token_callback,
        ))

        Ok(#(callback_context, filter_transform, insert_transform))
      }
    }
  }

  let context = #(
    callback_context,
    remove_fmi_transform,
    insert_specific_character_set_transform,
  )

  // Write File Preamble and File Meta Information tokens
  let preamble_token =
    list.repeat(<<0>>, 128)
    |> bit_array.concat
    |> p10_token.FilePreambleAndDICMPrefix
  use context <- result.try(process_token(context, preamble_token))
  let fmi_token =
    data_set
    |> data_set.file_meta_information
    |> p10_token.FileMetaInformation
  use context <- result.try(process_token(context, fmi_token))

  // Write main data set
  use context <- result.try(p10_token.data_elements_to_tokens(
    data_set,
    path,
    context,
    process_token,
  ))

  // Write end token
  use context <- result.map(process_token(context, p10_token.End))

  context.0
}

/// Converts a data set to DICOM P10 bytes. The generated P10 bytes are returned
/// via a callback.
///
pub fn data_set_to_bytes(
  data_set: DataSet,
  path: DataSetPath,
  context: a,
  bytes_callback: fn(a, BitArray) -> Result(a, P10Error),
  config: P10WriteConfig,
) -> Result(a, P10Error) {
  let write_context = new_write_context() |> with_config(config)

  let process_token = fn(context, token) {
    let #(context, write_context) = context

    use write_context <- result.try(write_token(write_context, token))
    let #(bytes, write_context) = read_bytes(write_context)

    use context <- result.map(list.try_fold(bytes, context, bytes_callback))

    #(context, write_context)
  }

  data_set_to_tokens(data_set, path, #(context, write_context), process_token)
  |> result.map(fn(x) { x.0 })
}

/// Sets the *'(0002,0001) File Meta Information Version'*, *'(0002,0012)
/// Implementation Class UID'* and *'(0002,0013) Implementation Version Name'*
/// values in the File Meta Information. This is done prior to serializing it to
/// bytes.
///
fn prepare_file_meta_information_token_data_set(
  file_meta_information: DataSet,
  implementation_class_uid: String,
  implementation_version_name: String,
) {
  let assert Ok(file_meta_information_version) =
    data_element_value.new_other_byte_string(<<0, 1>>)

  file_meta_information
  |> data_set.insert(
    dictionary.file_meta_information_version.tag,
    file_meta_information_version,
  )
  |> data_set.insert_string_value(dictionary.implementation_class_uid, [
    implementation_class_uid,
  ])
  |> result.try(
    data_set.insert_string_value(_, dictionary.implementation_version_name, [
      implementation_version_name,
    ]),
  )
}
