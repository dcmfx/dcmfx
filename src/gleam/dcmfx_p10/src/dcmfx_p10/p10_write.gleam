//// Functionality for serializing data sets and streams of DICOM P10 parts into
//// DICOM P10 bytes.

import dcmfx_core/data_element_tag
import dcmfx_core/data_element_value
import dcmfx_core/data_set.{type DataSet}
import dcmfx_core/data_set_path.{type DataSetPath}
import dcmfx_core/dictionary
import dcmfx_core/transfer_syntax.{
  type Endianness, type TransferSyntax, BigEndian, LittleEndian,
}
import dcmfx_core/value_representation
import dcmfx_p10/internal/data_element_header.{
  type DataElementHeader, DataElementHeader,
}
import dcmfx_p10/internal/value_length
import dcmfx_p10/internal/zlib.{type ZlibStream}
import dcmfx_p10/internal/zlib/flush_command
import dcmfx_p10/p10_error.{type P10Error}
import dcmfx_p10/p10_part.{type P10Part}
import dcmfx_p10/transforms/p10_filter_transform.{type P10FilterTransform}
import dcmfx_p10/transforms/p10_insert_transform.{type P10InsertTransform}
import dcmfx_p10/uids
import gleam/bit_array
import gleam/bool
import gleam/int
import gleam/list
import gleam/option.{type Option, None, Some}
import gleam/result

/// Configuration used when writing DICOM P10 data. The following config is
/// available:
///
/// ### `zlib_compression_level: Int`
///
/// The zlib compression level to use when the transfer syntax being used is
/// deflated. There are only three deflated transfer syntaxes: 'Deflated
/// Explicit VR Little Endian', 'JPIP Referenced Deflate', and 'JPIP HTJ2K
/// Referenced Deflate'.
///
/// The level ranges from 0, meaning no compression, through to 9, which gives
/// the best compression at the cost of speed.
///
/// Default: 6.
///
pub type P10WriteConfig {
  P10WriteConfig(zlib_compression_level: Int)
}

/// Returns the default write config.
///
pub fn default_config() -> P10WriteConfig {
  P10WriteConfig(zlib_compression_level: 6)
}

/// A write context holds the current state of an in-progress DICOM P10 write.
/// DICOM P10 parts are written to a write context with `write_part()`, and
/// output P10 bytes are returned by `read_bytes()`.
///
pub opaque type P10WriteContext {
  P10WriteContext(
    config: P10WriteConfig,
    p10_bytes: List(BitArray),
    p10_total_byte_count: Int,
    is_ended: Bool,
    transfer_syntax: TransferSyntax,
    zlib_stream: Option(ZlibStream),
    path: DataSetPath,
    sequence_item_counts: List(Int),
  )
}

/// Creates a new write context for writing DICOM P10 data.
///
pub fn new_write_context() -> P10WriteContext {
  P10WriteContext(
    config: default_config(),
    p10_bytes: [],
    p10_total_byte_count: 0,
    is_ended: False,
    transfer_syntax: transfer_syntax.implicit_vr_little_endian,
    zlib_stream: None,
    path: data_set_path.new(),
    sequence_item_counts: [],
  )
}

/// Updates the config for a write context.
///
pub fn with_config(
  context: P10WriteContext,
  config: P10WriteConfig,
) -> P10WriteContext {
  // Clamp zlib compression level to the valid range
  let config =
    P10WriteConfig(zlib_compression_level: int.clamp(
      config.zlib_compression_level,
      0,
      9,
    ))

  P10WriteContext(..context, config: config)
}

/// Reads the current DICOM P10 bytes available out of a write context. These
/// are the bytes generated by recent calls to `write_part()`.
///
pub fn read_bytes(
  context: P10WriteContext,
) -> #(List(BitArray), P10WriteContext) {
  let p10_bytes = list.reverse(context.p10_bytes)

  #(p10_bytes, P10WriteContext(..context, p10_bytes: []))
}

/// Writes a DICOM P10 part to a write context. On success an updated write
/// context is returned. Use `read_bytes()` to get the new DICOM P10 bytes
/// generated as a result of writing this part.
///
pub fn write_part(
  context: P10WriteContext,
  part: P10Part,
) -> Result(P10WriteContext, P10Error) {
  use <- bool.guard(
    context.is_ended,
    Error(p10_error.PartStreamInvalid(
      when: "Writing DICOM P10 part",
      details: "Received a further DICOM P10 part after the write was completed",
      part:,
    )),
  )

  case part {
    // When the File Meta Information part is received, check it for a transfer
    // syntax value that should be put onto the write context, and start a zlib
    // compressor if the transfer syntax is deflated
    p10_part.FileMetaInformation(file_meta_information) -> {
      // Read the transfer syntax UID
      let transfer_syntax_uid =
        file_meta_information
        |> data_set.get_string(dictionary.transfer_syntax_uid.tag)
        |> result.unwrap(transfer_syntax.implicit_vr_little_endian.uid)

      // Map UID to a known transfer syntax
      let new_transfer_syntax =
        transfer_syntax_uid
        |> transfer_syntax.from_uid
        |> result.map_error(fn(_) {
          p10_error.TransferSyntaxNotSupported(transfer_syntax_uid)
        })
      use new_transfer_syntax <- result.try(new_transfer_syntax)

      // If this is a deflated transfer syntax then start a zlib compressor.
      // The window bits of -15 stops the zlib header and checksum appearing in
      // the output.
      let zlib_stream = case new_transfer_syntax.is_deflated {
        True -> {
          let stream = zlib.open()
          zlib.deflate_init(
            stream,
            context.config.zlib_compression_level,
            zlib.Deflated,
            -15,
            8,
            zlib.Default,
          )

          Some(stream)
        }

        _ -> None
      }

      let new_context =
        P10WriteContext(
          ..context,
          transfer_syntax: new_transfer_syntax,
          zlib_stream: zlib_stream,
        )

      use part_bytes <- result.map(part_to_bytes(part, new_context))

      P10WriteContext(
        ..new_context,
        p10_bytes: [part_bytes, ..new_context.p10_bytes],
        p10_total_byte_count: context.p10_total_byte_count
          + bit_array.byte_size(part_bytes),
      )
    }

    // When the end part is received, update the flag on the write context and
    // flush all remaining data out of the zlib stream if one is in use
    p10_part.End ->
      case context.zlib_stream {
        Some(zlib_stream) -> {
          let data =
            zlib_stream
            |> zlib.deflate(<<>>, flush_command.Finish)
            |> bit_array.concat

          P10WriteContext(
            ..context,
            p10_bytes: [data, ..context.p10_bytes],
            p10_total_byte_count: context.p10_total_byte_count
              + bit_array.byte_size(data),
            is_ended: True,
            zlib_stream: None,
          )
          |> Ok
        }

        None -> Ok(P10WriteContext(..context, is_ended: True))
      }

    _ -> {
      // Update the current path
      let context =
        case part {
          p10_part.DataElementHeader(tag:, ..) ->
            data_set_path.add_data_element(context.path, tag)
            |> result.map(fn(path) { P10WriteContext(..context, path:) })

          p10_part.SequenceStart(tag:, ..) ->
            data_set_path.add_data_element(context.path, tag)
            |> result.map(fn(path) {
              P10WriteContext(..context, path:, sequence_item_counts: [
                0,
                ..context.sequence_item_counts
              ])
            })

          p10_part.SequenceItemStart | p10_part.PixelDataItem(..) -> {
            let assert [count, ..rest] = context.sequence_item_counts

            data_set_path.add_sequence_item(context.path, count)
            |> result.map(fn(path) {
              let sequence_item_counts = [count + 1, ..rest]

              P10WriteContext(..context, path:, sequence_item_counts:)
            })
          }

          _ -> Ok(context)
        }
        |> result.map_error(fn(_) {
          p10_error.PartStreamInvalid(
            when: "Writing part to context",
            details: "The data set path is not in a valid state for this part",
            part:,
          )
        })

      use context <- result.try(context)

      // Convert part to bytes
      use part_bytes <- result.try(part_to_bytes(part, context))

      // Update the current path
      let context =
        case part {
          p10_part.DataElementValueBytes(bytes_remaining: 0, ..)
          | p10_part.SequenceItemDelimiter ->
            data_set_path.pop(context.path)
            |> result.map(fn(path) { P10WriteContext(..context, path:) })

          p10_part.SequenceDelimiter -> {
            let assert Ok(sequence_item_counts) =
              list.rest(context.sequence_item_counts)

            data_set_path.pop(context.path)
            |> result.map(fn(path) {
              P10WriteContext(..context, path:, sequence_item_counts:)
            })
          }

          _ -> Ok(context)
        }
        |> result.map_error(fn(_) {
          p10_error.PartStreamInvalid(
            when: "Writing part to context",
            details: "The data set path is empty",
            part:,
          )
        })

      use context <- result.map(context)

      // If a zlib stream is active then pass the P10 bytes through it
      case context.zlib_stream {
        Some(zlib_stream) -> {
          // Add bytes to the zlib compressor and read back any compressed data
          let data =
            zlib_stream
            |> zlib.deflate(part_bytes, flush_command.None)
            |> bit_array.concat

          P10WriteContext(
            ..context,
            p10_bytes: [data, ..context.p10_bytes],
            p10_total_byte_count: context.p10_total_byte_count
              + bit_array.byte_size(data),
          )
        }

        None ->
          P10WriteContext(
            ..context,
            p10_bytes: [part_bytes, ..context.p10_bytes],
            p10_total_byte_count: context.p10_total_byte_count
              + bit_array.byte_size(part_bytes),
          )
      }
    }
  }
}

/// Converts a single DICOM P10 part to raw DICOM P10 bytes.
///
fn part_to_bytes(
  part: P10Part,
  context: P10WriteContext,
) -> Result(BitArray, P10Error) {
  let transfer_syntax = context.transfer_syntax

  case part {
    p10_part.FilePreambleAndDICMPrefix(preamble) -> {
      let preamble_length = bit_array.byte_size(preamble)

      case preamble_length {
        128 -> Ok(bit_array.append(preamble, <<"DICM">>))
        _ ->
          Error(p10_error.DataInvalid(
            "Serializing File Preamble",
            "Preamble data must be 128 bytes in length but "
              <> int.to_string(preamble_length)
              <> " bytes were supplied",
            context.path,
            context.p10_total_byte_count,
          ))
      }
    }

    p10_part.FileMetaInformation(file_meta_information) -> {
      let fmi_bytes =
        file_meta_information
        |> prepare_file_meta_information_part_data_set
        |> data_set.map(fn(tag, value) {
          let vr = data_element_value.value_representation(value)

          let value_bytes =
            value
            |> data_element_value.bytes
            |> result.replace_error(p10_error.DataInvalid(
              "Serializing File Meta Information",
              "Tag '"
                <> data_element_tag.to_string(tag)
                <> "' with value representation '"
                <> value_representation.to_string(vr)
                <> "' is not allowed in File Meta Information",
              context.path,
              context.p10_total_byte_count,
            ))
          use value_bytes <- result.try(value_bytes)

          let value_length = bit_array.byte_size(value_bytes)

          let header_bytes =
            DataElementHeader(tag, Some(vr), value_length.new(value_length))
            |> data_element_header_to_bytes(LittleEndian, context)
          use header_bytes <- result.try(header_bytes)

          Ok(bit_array.append(header_bytes, value_bytes))
        })
        |> result.all
        |> result.map(bit_array.concat)

      use fmi_bytes <- result.map(fmi_bytes)

      // Construct the File Meta Information Group Length data
      let fmi_length = bit_array.byte_size(fmi_bytes)
      let fmi_length_bytes = <<
        2:16-little,
        0:16,
        "UL",
        4:16-little,
        fmi_length:32-little,
      >>

      bit_array.concat([fmi_length_bytes, fmi_bytes])
    }

    p10_part.DataElementHeader(tag, vr, length) -> {
      let vr = case transfer_syntax.vr_serialization {
        transfer_syntax.VrExplicit -> Some(vr)
        transfer_syntax.VrImplicit -> None
      }

      DataElementHeader(tag, vr, value_length.new(length))
      |> data_element_header_to_bytes(transfer_syntax.endianness, context)
    }

    p10_part.DataElementValueBytes(vr, data, _) ->
      case transfer_syntax.endianness {
        LittleEndian -> data
        BigEndian -> value_representation.swap_endianness(vr, data)
      }
      |> Ok

    p10_part.SequenceStart(tag, vr) -> {
      let vr = case transfer_syntax.vr_serialization {
        transfer_syntax.VrExplicit -> Some(vr)
        transfer_syntax.VrImplicit -> None
      }

      DataElementHeader(tag, vr, value_length.Undefined)
      |> data_element_header_to_bytes(transfer_syntax.endianness, context)
    }

    p10_part.SequenceDelimiter ->
      DataElementHeader(
        dictionary.sequence_delimitation_item.tag,
        None,
        value_length.zero,
      )
      |> data_element_header_to_bytes(transfer_syntax.endianness, context)

    p10_part.SequenceItemStart ->
      DataElementHeader(dictionary.item.tag, None, value_length.Undefined)
      |> data_element_header_to_bytes(transfer_syntax.endianness, context)

    p10_part.SequenceItemDelimiter ->
      DataElementHeader(
        dictionary.item_delimitation_item.tag,
        None,
        value_length.zero,
      )
      |> data_element_header_to_bytes(transfer_syntax.endianness, context)

    p10_part.PixelDataItem(length) ->
      DataElementHeader(dictionary.item.tag, None, value_length.new(length))
      |> data_element_header_to_bytes(transfer_syntax.endianness, context)

    p10_part.End -> Ok(<<>>)
  }
}

/// Serializes a data element header to a `BitArray`. If a VR is not supplied
/// then implicit VR encoding will be used.
///
@internal
pub fn data_element_header_to_bytes(
  header: DataElementHeader,
  endianness: Endianness,
  context: P10WriteContext,
) -> Result(BitArray, P10Error) {
  let length = value_length.to_int(header.length)

  use <- bool.guard(
    length < 0,
    Error(p10_error.DataInvalid(
      "Serializing data element header",
      "Length is negative",
      context.path,
      context.p10_total_byte_count,
    )),
  )

  let tag_bytes = case endianness {
    LittleEndian -> <<header.tag.group:16-little, header.tag.element:16-little>>
    BigEndian -> <<header.tag.group:16-big, header.tag.element:16-big>>
  }

  case header.vr {
    // Write with implicit VR
    None ->
      case endianness {
        LittleEndian -> Ok(<<tag_bytes:bits, length:32-little>>)
        BigEndian -> Ok(<<tag_bytes:bits, length:32-big>>)
      }

    // Write with explicit VR
    Some(vr) -> {
      let length = value_length.to_int(header.length)

      let length_bytes = case data_element_header.value_length_size(vr) {
        data_element_header.ValueLengthU16 ->
          case length > 0xFFFF {
            True ->
              p10_error.DataInvalid(
                "Serializing data element header",
                "Length "
                  <> int.to_string(length)
                  <> " exceeds the maximum of 2^16 - 1 bytes",
                context.path,
                context.p10_total_byte_count,
              )
              |> Error

            False ->
              case endianness {
                LittleEndian -> Ok(<<length:16-little>>)
                BigEndian -> Ok(<<length:16-big>>)
              }
          }

        data_element_header.ValueLengthU32 ->
          case length > 0xFFFFFFFF {
            True ->
              p10_error.DataInvalid(
                "Serializing data element header",
                "Length "
                  <> int.to_string(length)
                  <> " exceeds the maximum of 0xFFFFFFFF",
                context.path,
                context.p10_total_byte_count,
              )
              |> Error

            False ->
              case endianness {
                LittleEndian -> Ok(<<0, 0, length:32-little>>)
                BigEndian -> Ok(<<0, 0, length:32-big>>)
              }
          }
      }

      use length_bytes <- result.try(length_bytes)

      Ok(<<
        tag_bytes:bits,
        value_representation.to_string(vr):utf8,
        length_bytes:bits,
      >>)
    }
  }
}

/// Converts a data set to DICOM P10 parts. The generated P10 parts are returned
/// via a callback.
///
pub fn data_set_to_parts(
  data_set: DataSet,
  callback_context: a,
  part_callback: fn(a, P10Part) -> Result(a, e),
) -> Result(a, e) {
  // Create filter transform that removes File Meta Information data elements
  // from the data set's part stream
  let remove_fmi_transform =
    p10_filter_transform.new(fn(tag, _, _) { tag.group != 2 }, False)

  // Create insert transform to add the '(0008,0005) SpecificCharacterSet' data
  // element into the data set's part stream, specifying UTF-8 (ISO_IR 192)
  let assert Ok(data_elements_to_insert) =
    data_set.new()
    |> data_set.insert_string_value(dictionary.specific_character_set, [
      "ISO_IR 192",
    ])
  let insert_specific_character_set_transform =
    p10_insert_transform.new(data_elements_to_insert)

  // Create a function that passes parts through the above two transforms and
  // then to the callback
  let process_part = fn(
    context: #(a, P10FilterTransform, P10InsertTransform),
    part: P10Part,
  ) {
    case p10_filter_transform.add_part(context.1, part) {
      #(False, filter_transform) ->
        Ok(#(context.0, filter_transform, context.2))

      #(True, filter_transform) -> {
        let #(parts, insert_transform) =
          p10_insert_transform.add_part(context.2, part)

        use callback_context <- result.try(list.try_fold(
          parts,
          context.0,
          part_callback,
        ))

        Ok(#(callback_context, filter_transform, insert_transform))
      }
    }
  }

  let context = #(
    callback_context,
    remove_fmi_transform,
    insert_specific_character_set_transform,
  )

  // Write File Preamble and File Meta Information parts
  let preamble_part =
    list.repeat(<<0>>, 128)
    |> bit_array.concat
    |> p10_part.FilePreambleAndDICMPrefix
  use context <- result.try(process_part(context, preamble_part))
  let fmi_part =
    data_set
    |> data_set.file_meta_information
    |> p10_part.FileMetaInformation
  use context <- result.try(process_part(context, fmi_part))

  // Write main data set
  use context <- result.try(p10_part.data_elements_to_parts(
    data_set,
    context,
    process_part,
  ))

  // Write end part
  use context <- result.map(process_part(context, p10_part.End))

  context.0
}

/// Converts a data set to DICOM P10 bytes. The generated P10 bytes are returned
/// via a callback.
///
pub fn data_set_to_bytes(
  data_set: DataSet,
  context: a,
  bytes_callback: fn(a, BitArray) -> Result(a, P10Error),
  config: P10WriteConfig,
) -> Result(a, P10Error) {
  let write_context = new_write_context() |> with_config(config)

  let process_part = fn(context, part) {
    let #(context, write_context) = context

    use write_context <- result.try(write_part(write_context, part))
    let #(bytes, write_context) = read_bytes(write_context)

    use context <- result.map(list.try_fold(bytes, context, bytes_callback))

    #(context, write_context)
  }

  data_set_to_parts(data_set, #(context, write_context), process_part)
  |> result.map(fn(x) { x.0 })
}

/// Sets the *'(0002,0001) File Meta Information Version'*, *'(0002,0012)
/// Implementation Class UID'* and *'(0002,0013) Implementation Version Name'*
/// values in the File Meta Information. This is done prior to serializing it to
/// bytes.
///
fn prepare_file_meta_information_part_data_set(file_meta_information: DataSet) {
  let assert Ok(file_meta_information_version) =
    data_element_value.new_other_byte_string(<<0, 1>>)
  let assert Ok(implementation_class_uid) =
    data_element_value.new_unique_identifier([
      uids.dcmfx_implementation_class_uid,
    ])
  let assert Ok(implementation_version_name) =
    data_element_value.new_short_string([uids.dcmfx_implementation_version_name])

  file_meta_information
  |> data_set.insert(
    dictionary.file_meta_information_version.tag,
    file_meta_information_version,
  )
  |> data_set.insert(
    dictionary.implementation_class_uid.tag,
    implementation_class_uid,
  )
  |> data_set.insert(
    dictionary.implementation_version_name.tag,
    implementation_version_name,
  )
}
